{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Business Questions to Find answers to"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question I want to provide answers to:\n",
    " - What are the most popular Airbnb destinations?\n",
    " - What factors influence cost of Airbnb rental costs?\n",
    " - What contributes to good Airbnb ratings?\n",
    " - Does cancelation policy affect reviews?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Understanding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings = pd.read_csv(\"seattle/listings.csv\")\n",
    "print(\"\\nShape of dataset: \",listings.shape,\"\\n\")\n",
    "listings.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "The output from this cell reveals there are a number of columns/rows \n",
    "that have nan/null values and hence require dropping(removing).\n",
    "\"\"\"\n",
    "listings.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning\n",
    "\n",
    "The dataset has way so many features and not all of them are relevant for the analysis we want to do. This means we have to drop some columns(features) and rows however we'll need a good reason for each drop. \n",
    "I have listed categories of reasons and how to identify what to drop below:<br/>\n",
    "<ol>\n",
    "<li>Columns with non-unique values: These columns have a single repeated for all the rows of that columns and hence they do not add anything significant to the dataset.</li><br/>\n",
    "<li>Rows that have about 70% of values as Nan, None or empty strings: 70% is a huge chunk of the dataset that we cannot impute data for and wouldn't make sense to maintain</li><br/>\n",
    "</ol>\n",
    "\n",
    "Having followed thr above process will reduce the features to 80 from 92."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the threshold count for dropping columns\n",
    "threshold_col = len(listings) * 0.7\n",
    "\n",
    "# Replace empty and \"none\" values with NaN\n",
    "listings_replaced = listings.replace(['', 'none'], np.nan)\n",
    "\n",
    "# Count the number of NaN values in each column\n",
    "na_counts = listings_replaced.isna().sum()\n",
    "\n",
    "# Get the column indices where the count exceeds or equals the threshold\n",
    "columns_to_drop_1 = na_counts[na_counts >= threshold_col].index\n",
    "\n",
    "# Drop the columns\n",
    "listings = listings_replaced.drop(columns=columns_to_drop_1)\n",
    "\n",
    "# Get the unique value counts for each column\n",
    "value_counts = listings.nunique()\n",
    "\n",
    "# Get the column names where all values are the same\n",
    "columns_to_drop_2 = value_counts[value_counts == 1].index\n",
    "\n",
    "# Drop the non-unique columns of this dataset\n",
    "listings = listings.drop(columns=columns_to_drop_2)\n",
    "columns_to_drop = columns_to_drop_1.to_list() + columns_to_drop_2.to_list()\n",
    "print(\"Dropped columns are :\", columns_to_drop)\n",
    "print(\"listings shape: \", listings.shape)\n",
    "listings.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will print out all the columns of the dataset.\n",
    "You can go through them manually to identify which features are most relevant to our analysis and which aren't.\n",
    "This way we can drop more features and have a more meaningful dataset to work with.\n",
    "<br/>\n",
    "\n",
    "This could mean checking out each column to know what datatype type it is and if the value thereof is relevant. \n",
    "You'll find out that most of the Ids are not relevant for the analysis we want to make. Also some other columns like summary, description, space and other features where we are mostly dealing with texts/urls that can't be grouped into some sort of categorical variables need to be dropped as well because they are all unique for each and every role of that columns and hence doesn't pose anything interesting to study or analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-numeric characters from the 'pricing' column\n",
    "listings['price'] = listings['price'].apply(lambda x: re.sub(r'[^\\d.]+', '', str(x)))\n",
    "\n",
    "# Print the column names\n",
    "print(listings.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection & Visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns of interest, including 'price'\n",
    "\n",
    "columns_of_interest = [\n",
    "    'host_is_superhost',\n",
    "    'neighbourhood_cleansed',\n",
    "    'accommodates',\n",
    "    'bedrooms',\n",
    "    'bathrooms',\n",
    "    'is_location_exact',\n",
    "    'review_scores_rating',\n",
    "    'property_type',\n",
    "    'room_type',\n",
    "    'beds',\n",
    "    'bed_type',\n",
    "    'number_of_reviews',\n",
    "    'instant_bookable',\n",
    "    'review_scores_accuracy',\n",
    "    'review_scores_cleanliness',\n",
    "    'review_scores_communication',\n",
    "    'review_scores_value',\n",
    "    'cancellation_policy',\n",
    "    'availability_365',\n",
    "    'price'\n",
    "]\n",
    "\n",
    "len(columns_of_interest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heapmap showing correlation between selected non-categotical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the data to include only the columns of interest\n",
    "listings_relevant = listings.loc[:, columns_of_interest]\n",
    "\n",
    "# Remove non-numeric characters from the 'pricing' column\n",
    "listings_relevant['price'] = listings_relevant['price'].apply(lambda x: re.sub(r'[^\\d.]+', '', str(x))).astype(float)\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = listings_relevant.corr()\n",
    "\n",
    "# Visualize the correlations using a heatmap\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(correlation_matrix, cmap='Blues', annot=True, fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 20 features, only 12 were accounted for in the heapmap above because the remaining values are categorical and need to be treated differently."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with categorical variables & improperly represented Boolean Variables in the listings dataset:\n",
    "\n",
    "This is still some part of the data cleaning process as we still do not have the dataset in the format it should be before completely anlysing. A few things:</br>\n",
    "<ol>\n",
    "<li>There are boolean values represent as strings in the form of 't' or 'f'. These should be replace with actual Boolean values of 0 and 1.</li>\n",
    "<li>Idnetify the catogorical variables and treat them appropriately</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns containing 'f' or 't' values\n",
    "boolean_cols = [col for col in listings_relevant if listings[col].isin(['f','t', np.nan]).all()]\n",
    "\n",
    "# Convert 'f' to False and 't' to True in the identified columns\n",
    "listings_relevant[boolean_cols] = listings_relevant[boolean_cols].replace({'f': False, 't': True})\n",
    "\n",
    "#Remove all the nan values in the dataset and convert the True/False values to 0/1:\n",
    "listings_relevant = listings_relevant.dropna(subset=[b for b in boolean_cols]).astype({b: int for b in boolean_cols})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the variables plot based on their count/frequency of occurences\n",
    "variables_to_plot = ['host_is_superhost', 'is_location_exact', 'property_type',\n",
    "                     'room_type', 'bed_type', 'instant_bookable', \n",
    "                     'cancellation_policy','neighbourhood_cleansed']\n",
    "\n",
    "# Group by each column and plot against the price\n",
    "for column in variables_to_plot:\n",
    "    status_city = listings_relevant[column].value_counts()\n",
    "    (status_city/listings_relevant.shape[0]).plot(kind=\"bar\");\n",
    "    plt.xlabel(column)\n",
    "    plt.title(f'Value count of {column}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-numeric characters from the 'price' column\n",
    "listings_relevant['price'] = listings_relevant['price'].apply(lambda x: re.sub(r'[^\\d.]+', '', str(x)))\n",
    "\n",
    "# Select the variables to normalize and plot against the price\n",
    "variables_to_plot = ['host_is_superhost', 'is_location_exact', 'property_type',\n",
    "                     'room_type', 'bed_type', 'instant_bookable',\n",
    "                     'cancellation_policy', 'neighbourhood_cleansed']\n",
    "\n",
    "# Calculate the number of rows and columns for the subplots\n",
    "n_plots = len(variables_to_plot)\n",
    "n_rows = math.ceil(n_plots / 2)\n",
    "n_cols = 2\n",
    "# Create the subplots\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, 30))\n",
    "\n",
    "# Iterate over the variables and create the subplots\n",
    "for i, column in enumerate(variables_to_plot):\n",
    "    row = i // n_cols\n",
    "    col = i % n_cols\n",
    "\n",
    "    # Plot the value counts\n",
    "    status_city = listings_relevant[column].value_counts()\n",
    "    (status_city / listings_relevant.shape[0]).plot(kind=\"bar\", ax=axes[row, col])\n",
    "    axes[row, col].set_xlabel(column)\n",
    "    axes[row, col].set_ylabel('Frequency')\n",
    "    axes[row, col].set_title(f'Value count of {column}')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(hspace=0.5, wspace=0.3)  # Increase the spacing between subplots\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'price' column to float\n",
    "listings_relevant['price'] = listings_relevant['price'].astype(float)\n",
    "\n",
    "# Define the columns to group by\n",
    "group_by_columns = ['host_is_superhost', 'is_location_exact', 'property_type',\n",
    "                    'room_type', 'bed_type', 'instant_bookable', 'cancellation_policy','neighbourhood_cleansed']\n",
    "\n",
    "for col in group_by_columns:\n",
    "    # Group the data by the specified columns and calculate the average pricing\n",
    "    grouped_data = listings_relevant.groupby(col)['price'].mean().reset_index()\n",
    "\n",
    "    # Sort the data in ascending order based on the 'price' column\n",
    "    sorted_data = grouped_data.sort_values('price')\n",
    "\n",
    "    # Plot the grouped data\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=sorted_data, x=col, y='price', palette='viridis')\n",
    "    plt.xlabel(f'{col}')\n",
    "    plt.ylabel('Average Pricing')\n",
    "    plt.title(f'Average Pricing by {col}')\n",
    "\n",
    "    # Get the x-axis tick labels\n",
    "    x_ticks_labels = plt.gca().get_xticklabels()\n",
    "    # Slant the x-axis tick labels by the specified angle\n",
    "    plt.gca().set_xticklabels(x_ticks_labels, rotation=90)\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the frequency of each neighborhood\n",
    "neighborhood_counts = listings_relevant['neighbourhood_cleansed'].value_counts()\n",
    "\n",
    "# Plot the frequency as a pie chart\n",
    "plt.figure(figsize=(8, 8))  # Adjust the figure size if needed\n",
    "plt.pie(neighborhood_counts, labels=neighborhood_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Frequency of Neighborhoods')\n",
    "\n",
    "# Show the pie chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the value counts of each neighborhood\n",
    "neighborhood_counts = listings_relevant['neighbourhood_cleansed'].value_counts()\n",
    "\n",
    "# Calculate the threshold for the top neighborhoods\n",
    "threshold = int(len(neighborhood_counts) * 0.1)\n",
    "\n",
    "# Get the top neighborhoods based on value counts\n",
    "top_neighborhoods = neighborhood_counts.head(threshold)\n",
    "\n",
    "# Filter the data for the top neighborhoods\n",
    "filtered_data = listings_relevant[listings_relevant['neighbourhood_cleansed'].isin(top_neighborhoods.index)]\n",
    "\n",
    "# Set the color palette for grouping\n",
    "palette = sns.color_palette('viridis', len(filtered_data['neighbourhood_cleansed'].unique()))\n",
    "\n",
    "# Create the cluster plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_neighborhoods.plot(kind=\"bar\")\n",
    "# sns.stripplot(data=filtered_data, x='neighbourhood_cleansed', y='price', hue='neighbourhood_cleansed', palette=palette, dodge=True)\n",
    "plt.xlabel('Neighbourhood Cleansed')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Bar chart: Pricing by top Neighbourhood Cleansed (Grouped)')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join([f\"{column} : {listings_relevant[column].nunique()}\" for column in listings_relevant.select_dtypes(exclude=['int']).columns]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_relevant = listings_relevant.dropna(axis=0)\n",
    "# Filter columns that start with 'review_'\n",
    "review_columns = listings_relevant.filter(regex=r'^review_')\n",
    "\n",
    "# Create a new DataFrame with the review columns\n",
    "reviews = listings_relevant[review_columns.columns]\n",
    "\n",
    "# Create a new DataFrame without the review columns\n",
    "listings_relevant = listings_relevant.drop(review_columns, axis=1)\n",
    "\n",
    "listings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter columns that start with 'review_'\n",
    "review_columns = listings_relevant.filter(regex=r'^review_')\n",
    "\n",
    "# Create a new DataFrame with the review columns\n",
    "reviews = listings_relevant[review_columns.columns]\n",
    "\n",
    "# Create a new DataFrame without the review columns\n",
    "listings = listings_relevant.drop(review_columns.columns, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining columns in the calendar file do not have Nan values. However, there is common column names \"date\" in the calendar csv file and the reviews csv file. You may one to rename both as they do not represent the same kind of date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_relevant.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame for encoded data\n",
    "encoded_data = listings_relevant.copy()\n",
    "\n",
    "# Identify categorical variables based on data type\n",
    "categorical_vars = listings_relevant.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Perform one-hot encoding for categorical variables\n",
    "encoded_data = pd.get_dummies(encoded_data, columns=categorical_vars, drop_first=True)\n",
    "\n",
    "# Print the resulting encoded data\n",
    "encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform descriptive statistics and data visualization\n",
    "def explore_data(dataset):\n",
    "    \"\"\"\n",
    "    Perform descriptive statistics and data visualization for the Airbnb dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset (pandas.DataFrame): The input dataset to explore.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Perform descriptive statistics\n",
    "    summary_stats = dataset.describe()\n",
    "    \n",
    "    # Print the summary statistics\n",
    "    print(\"Summary Statistics:\")\n",
    "    print(summary_stats)\n",
    "    \n",
    "    # Perform data visualization\n",
    "    \n",
    "    # Histogram of prices\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    dataset['price'].hist(bins=30, color='blue', alpha=0.7)\n",
    "    plt.xlabel('Price')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Prices')\n",
    "    plt.show()\n",
    "    \n",
    "    # Scatter plot of price vs. number_of_reviews\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(dataset['number_of_reviews'], dataset['price'], color='green', alpha=0.5)\n",
    "    plt.xlabel('Number of Reviews')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title('Price vs. Number of Reviews')\n",
    "    plt.show()\n",
    "\n",
    "# Call the explore_data function with the loaded dataset\n",
    "explore_data(listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvml-7FaA3ELl-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
